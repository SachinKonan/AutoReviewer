============================================================
OUTPUT TYPE TESTS
============================================================
Dataset: /n/fs/vision-mix/jl0796/iclr-reviews-2020-2026
Model: Qwen/Qwen2.5-0.5B-Instruct
Samples per test: 3

============================================================
TEST: OutputType.BINARY - Accept/Reject Prediction
============================================================
Splits to load:  ['2020']
Loading 2020 from local path
Loaded 2020 of length 2198

DEBUG: Processing dataset, type=<class 'datasets.arrow_dataset.Dataset'>, len=2198
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
Loaded 3 submissions

Instruction:
Based on the paper information provided, predict whether this paper will be accepted or rejected at ICLR.

Provide your response in the following format:
Reasoning: [Your reasoning about the paper's quality, novelty, significance, and fit for the conference]
Answer: [Either 'Accept' or 'Reject']
  ryxz8CVYDH: decision=Accept (Poster) -> binary=Accept
  ryxyCeHtPB: decision=Accept (Poster) -> binary=Accept
  ryxtWgSKPB: decision=Reject -> binary=Reject
INFO 12-24 22:54:08 [utils.py:253] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-24 22:54:08 [model.py:631] Resolved architecture: Qwen2ForCausalLM
INFO 12-24 22:54:08 [model.py:1745] Using max model len 32768
INFO 12-24 22:54:08 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:09 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:10 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.103:48271 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:10 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:10 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:11 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:11 [cuda.py:427] Using FLASH_ATTN backend.
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:11 [weight_utils.py:481] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.57it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.56it/s]
(EngineCore_DP0 pid=2786480) 
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:11 [default_loader.py:314] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:12 [gpu_model_runner.py:3338] Model loading took 0.9269 GiB memory and 0.873910 seconds
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:17 [backends.py:631] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/adbeadfbd0/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:17 [backends.py:647] Dynamo bytecode transform time: 5.00 s
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:19 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.997 s
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:20 [monitor.py:34] torch.compile takes 6.99 s in total
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:21 [gpu_worker.py:359] Available KV cache memory: 37.59 GiB
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:21 [kv_cache_utils.py:1229] GPU KV cache size: 3,284,656 tokens
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:21 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 100.24x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████| 51/51 [00:01<00:00, 32.98it/s]
Capturing CUDA graphs (decode, FULL): 100%|█████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 38.01it/s]
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:24 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took 0.46 GiB
(EngineCore_DP0 pid=2786480) INFO 12-24 22:54:24 [core.py:250] init engine (profile, create kv cache, warmup model) took 12.75 seconds
INFO 12-24 22:54:25 [llm.py:352] Supported tasks: ['generate']
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 191.65it/s]
Processed prompts: 100%|████████████████████| 3/3 [00:00<00:00,  7.82it/s, est. speed input: 12082.10 toks/s, output: 821.98 toks/s]

Predictions:
  ryxz8CVYDH: Accept (parse_success=True)
    Raw: Reasoning: The paper presents a novel zeroth-order optimization framework for learning to learn, whi...
  ryxyCeHtPB: Accept (parse_success=True)
    Raw: Reasoning: The paper presents a novel approach to transfer learning that uses attentive feature sele...
  ryxtWgSKPB: Accept (parse_success=True)
    Raw: Reasoning: The paper presents a novel approach to predicting quantum optical experiments using long ...

============================================================
TEST: OutputType.MULTICLASS - Decision Tier Prediction
============================================================
Splits to load:  ['2020']
Loading 2020 from local path
Loaded 2020 of length 2198

DEBUG: Processing dataset, type=<class 'datasets.arrow_dataset.Dataset'>, len=2198
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
Loaded 3 submissions

Instruction:
Based on the paper information provided, predict the final decision for this ICLR submission.

Possible decisions: 'Reject', 'Accept (Poster)', 'Accept (Spotlight)', 'Accept (Oral)'

Provide your response in the following format:
Reasoning: [Your reasoning about the paper's quality, novelty, significance, and expected tier]
Answer: [Exact decision from the list above]
  ryxz8CVYDH: decision=Accept (Poster) -> multiclass=Accept (Poster)
  ryxyCeHtPB: decision=Accept (Poster) -> multiclass=Accept (Poster)
  ryxtWgSKPB: decision=Reject -> multiclass=Reject
INFO 12-24 22:54:27 [utils.py:253] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-24 22:54:27 [model.py:631] Resolved architecture: Qwen2ForCausalLM
INFO 12-24 22:54:27 [model.py:1745] Using max model len 32768
INFO 12-24 22:54:27 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:27 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:28 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.103:39563 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:28 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:29 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:29 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:29 [cuda.py:427] Using FLASH_ATTN backend.
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:30 [weight_utils.py:481] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.60it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.60it/s]
(EngineCore_DP0 pid=2786595) 
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:30 [default_loader.py:314] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:30 [gpu_model_runner.py:3338] Model loading took 0.9269 GiB memory and 0.948730 seconds
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:35 [backends.py:631] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/adbeadfbd0/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:35 [backends.py:647] Dynamo bytecode transform time: 4.85 s
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:38 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.824 s
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:39 [monitor.py:34] torch.compile takes 6.68 s in total
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:39 [gpu_worker.py:359] Available KV cache memory: 37.59 GiB
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:40 [kv_cache_utils.py:1229] GPU KV cache size: 3,284,656 tokens
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:40 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 100.24x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████| 51/51 [00:01<00:00, 33.00it/s]
Capturing CUDA graphs (decode, FULL): 100%|█████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 38.02it/s]
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:43 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took 0.46 GiB
(EngineCore_DP0 pid=2786595) INFO 12-24 22:54:43 [core.py:250] init engine (profile, create kv cache, warmup model) took 12.43 seconds
INFO 12-24 22:54:44 [llm.py:352] Supported tasks: ['generate']
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 191.93it/s]
Processed prompts: 100%|████████████████████| 3/3 [00:00<00:00,  7.89it/s, est. speed input: 12321.84 toks/s, output: 902.28 toks/s]

Predictions:
  ryxz8CVYDH: Accept (Spotlight) (parse_success=True)
    Raw: Reasoning: The paper presents a novel zeroth-order optimization framework for learning to learn, whi...
  ryxyCeHtPB: Accept (Spotlight) (parse_success=True)
    Raw: Reasoning: The paper presents a novel approach to transfer learning that uses attentive feature sele...
  ryxtWgSKPB: Accept (Spotlight) (parse_success=True)
    Raw: Reasoning: The paper presents a novel approach to predicting quantum optical experiments using long ...

============================================================
TEST: OutputType.CITATION_PERCENTILE - Citation Prediction
============================================================
Splits to load:  ['2020']
Loading 2020 from local path
Loaded 2020 of length 2198

DEBUG: Processing dataset, type=<class 'datasets.arrow_dataset.Dataset'>, len=2198
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
Loaded 3 submissions

Instruction:
Based on the paper information provided, predict its citation impact percentile.

The percentile should be between 0.0 and 1.0, where:
- 0.0 = bottom percentile (least cited papers)
- 0.25 = bottom quartile
- 0.5 = median citations
- 0.75 = top quartile
- 0.9 = top 10%
- 1.0 = top percentile (most cited papers)

Provide your response in the following format:
Reasoning: [Your assessment of the paper's potential impact, considering novelty, significance, clarity, and field]
Answer: [Decimal number between 0.0 and 1.0, e.g., 0.75]

Ground truth (if available):
  ryxz8CVYDH: citation_percentile=None
  ryxyCeHtPB: citation_percentile=None
  ryxtWgSKPB: citation_percentile=None
INFO 12-24 22:54:45 [utils.py:253] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-24 22:54:45 [model.py:631] Resolved architecture: Qwen2ForCausalLM
INFO 12-24 22:54:45 [model.py:1745] Using max model len 32768
INFO 12-24 22:54:45 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:46 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:51 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.103:42589 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:51 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:51 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:52 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:52 [cuda.py:427] Using FLASH_ATTN backend.
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:52 [weight_utils.py:481] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.47it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.47it/s]
(EngineCore_DP0 pid=2786803) 
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:52 [default_loader.py:314] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:53 [gpu_model_runner.py:3338] Model loading took 0.9269 GiB memory and 0.875057 seconds
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:58 [backends.py:631] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/adbeadfbd0/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2786803) INFO 12-24 22:54:58 [backends.py:647] Dynamo bytecode transform time: 4.90 s
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:00 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.888 s
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:01 [monitor.py:34] torch.compile takes 6.79 s in total
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:02 [gpu_worker.py:359] Available KV cache memory: 37.59 GiB
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:02 [kv_cache_utils.py:1229] GPU KV cache size: 3,284,656 tokens
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:02 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 100.24x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████| 51/51 [00:01<00:00, 32.63it/s]
Capturing CUDA graphs (decode, FULL): 100%|█████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 36.87it/s]
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:06 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took 0.46 GiB
(EngineCore_DP0 pid=2786803) INFO 12-24 22:55:06 [core.py:250] init engine (profile, create kv cache, warmup model) took 12.69 seconds
INFO 12-24 22:55:07 [llm.py:352] Supported tasks: ['generate']
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 537.23it/s]
Processed prompts: 100%|█████████████████████| 3/3 [00:00<00:00,  5.54it/s, est. speed input: 2182.39 toks/s, output: 750.24 toks/s]

Predictions:
  ryxz8CVYDH: 0.95 (parse_success=True)
    Raw: Reasoning: The paper proposes a novel approach to zeroth-order (ZO) optimization, which is a signifi...
  ryxyCeHtPB: 0.95 (parse_success=True)
    Raw: Reasoning: The paper proposes a method called AFDS (Attention Feature Distillation and Selection) th...
  ryxtWgSKPB: 0.75 (parse_success=True)
    Raw: Reasoning: The paper presents a novel approach to model quantum experiments using long short-term me...

============================================================
TEST: OutputType.MEAN_RATING - Reviewer Rating Prediction
============================================================
Splits to load:  ['2020']
Loading 2020 from local path
Loaded 2020 of length 2198

DEBUG: Processing dataset, type=<class 'datasets.arrow_dataset.Dataset'>, len=2198
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
Loaded 3 submissions

Instruction:
Based on the paper information provided, predict the expected mean reviewer rating on a scale of 1.0 to 10.0.

Rating scale interpretation:
- 1.0-3: Strong reject
- 4: Reject
- 5: Marginally below acceptance threshold
- 6: Marginally above acceptance threshold
- 7: Good paper, accept
- 8: Strong accept
- 9-10.0: Outstanding paper

Provide your response in the following format:
Reasoning: [Your assessment of the paper's quality and likely reception]
Answer: [Number between 1.0 and 10.0]

Ground truth (calculated from reviews):
  ryxz8CVYDH: mean_rating=6.67 (3 reviews)
  ryxyCeHtPB: mean_rating=6.67 (3 reviews)
  ryxtWgSKPB: mean_rating=1.67 (3 reviews)
INFO 12-24 22:55:08 [utils.py:253] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-24 22:55:09 [model.py:631] Resolved architecture: Qwen2ForCausalLM
INFO 12-24 22:55:09 [model.py:1745] Using max model len 32768
INFO 12-24 22:55:09 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:10 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:15 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.103:56769 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:15 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:17 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:19 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:19 [cuda.py:427] Using FLASH_ATTN backend.
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:19 [weight_utils.py:481] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.59it/s]
(EngineCore_DP0 pid=2787071) 
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:20 [default_loader.py:314] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:20 [gpu_model_runner.py:3338] Model loading took 0.9269 GiB memory and 2.315260 seconds
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:26 [backends.py:631] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/adbeadfbd0/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:26 [backends.py:647] Dynamo bytecode transform time: 5.19 s
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:28 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.822 s
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:29 [monitor.py:34] torch.compile takes 7.01 s in total
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:30 [gpu_worker.py:359] Available KV cache memory: 37.59 GiB
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:30 [kv_cache_utils.py:1229] GPU KV cache size: 3,284,656 tokens
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:30 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 100.24x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████| 51/51 [00:01<00:00, 33.54it/s]
Capturing CUDA graphs (decode, FULL): 100%|█████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 37.95it/s]
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:33 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took 0.46 GiB
(EngineCore_DP0 pid=2787071) INFO 12-24 22:55:33 [core.py:250] init engine (profile, create kv cache, warmup model) took 12.71 seconds
INFO 12-24 22:55:34 [llm.py:352] Supported tasks: ['generate']
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 542.58it/s]
Processed prompts: 100%|█████████████████████| 3/3 [00:00<00:00,  5.61it/s, est. speed input: 2107.24 toks/s, output: 775.94 toks/s]

Predictions:
  ryxz8CVYDH: 9.0 (parse_success=True)
    Raw: Reasoning: The paper proposes a novel approach to zeroth-order (ZO) optimization, where the gradient...
  ryxyCeHtPB: 10.0 (parse_success=True)
    Raw: Reasoning: The paper proposes AFDS, which is a method that dynamically selects and adjusts the impor...
  ryxtWgSKPB: 9.0 (parse_success=True)
    Raw: Reasoning: The paper presents a novel approach to model quantum experiments using long short-term me...

============================================================
TEST: BINARY with reasoning enabled
============================================================
Splits to load:  ['2020']
Loading 2020 from local path
Loaded 2020 of length 2198

DEBUG: Processing dataset, type=<class 'datasets.arrow_dataset.Dataset'>, len=2198
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
Loaded 3 submissions

Instruction:
Based on the paper information provided, predict whether this paper will be accepted or rejected at ICLR.

Provide your response in the following format:
Reasoning: [Your reasoning about the paper's quality, novelty, significance, and fit for the conference]
Answer: [Either 'Accept' or 'Reject']
INFO 12-24 22:55:35 [utils.py:253] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-24 22:55:35 [model.py:631] Resolved architecture: Qwen2ForCausalLM
INFO 12-24 22:55:35 [model.py:1745] Using max model len 32768
INFO 12-24 22:55:35 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:36 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.103:53377 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:38 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:38 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:38 [cuda.py:427] Using FLASH_ATTN backend.
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:39 [weight_utils.py:481] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.66it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.65it/s]
(EngineCore_DP0 pid=2787192) 
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:39 [default_loader.py:314] Loading weights took 0.20 seconds
(EngineCore_DP0 pid=2787192) INFO 12-24 22:55:39 [gpu_model_runner.py:3338] Model loading took 0.9269 GiB memory and 0.914257 seconds
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:02 [backends.py:631] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/adbeadfbd0/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:02 [backends.py:647] Dynamo bytecode transform time: 22.01 s
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:04 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.224 s
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:05 [monitor.py:34] torch.compile takes 24.23 s in total
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:06 [gpu_worker.py:359] Available KV cache memory: 37.59 GiB
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:06 [kv_cache_utils.py:1229] GPU KV cache size: 3,284,656 tokens
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:06 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 100.24x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████| 51/51 [00:01<00:00, 32.77it/s]
Capturing CUDA graphs (decode, FULL): 100%|█████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 37.81it/s]
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:10 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took 0.46 GiB
(EngineCore_DP0 pid=2787192) INFO 12-24 22:56:10 [core.py:250] init engine (profile, create kv cache, warmup model) took 30.20 seconds
INFO 12-24 22:56:11 [llm.py:352] Supported tasks: ['generate']
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 188.20it/s]
Processed prompts: 100%|████████████████████| 3/3 [00:00<00:00,  7.77it/s, est. speed input: 11992.70 toks/s, output: 815.90 toks/s]

Predictions with reasoning:

--- ryxz8CVYDH ---
Prediction: Accept
Reasoning: The paper presents a novel zeroth-order optimization framework for learning to learn, which is a significant advancement in the field of machine learning. The authors demonstrate that this framework o...
Parse success: True

--- ryxyCeHtPB ---
Prediction: Accept
Reasoning: The paper presents a novel approach to transfer learning that uses attentive feature selection and selective feature distillation to improve the performance of CNNs. The authors propose AFDS, which dy...
Parse success: True

--- ryxtWgSKPB ---
Prediction: Accept
Reasoning: The paper presents a novel approach to predicting quantum optical experiments using long short-term memory (LSTM) networks. The authors show that LSTM networks can effectively learn to predict the cha...
Parse success: True

============================================================
TEST: Evaluation Metrics
============================================================
Splits to load:  ['2020']
Loading 2020 from local path
Loaded 2020 of length 2198

DEBUG: Processing dataset, type=<class 'datasets.arrow_dataset.Dataset'>, len=2198
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
DEBUG: Got row with keys: ['submission_id', 'year', 'openreview_link', 'pdf_download_link', 'title']
Loaded 3 submissions
INFO 12-24 22:56:12 [utils.py:253] non-default args: {'trust_remote_code': True, 'disable_log_stats': True, 'model': 'Qwen/Qwen2.5-0.5B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-24 22:56:13 [model.py:631] Resolved architecture: Qwen2ForCausalLM
INFO 12-24 22:56:13 [model.py:1745] Using max model len 32768
INFO 12-24 22:56:13 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
        - Avoid using `tokenizers` before the fork if possible
        - Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:14 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-0.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-0.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-0.5B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.VLLM_COMPILE: 3>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['none'], 'splitting_ops': ['vllm::unified_attention', 'vllm::unified_attention_with_output', 'vllm::unified_mla_attention', 'vllm::unified_mla_attention_with_output', 'vllm::mamba_mixer2', 'vllm::mamba_mixer', 'vllm::short_conv', 'vllm::linear_attention', 'vllm::plamo2_mamba_mixer', 'vllm::gdn_attention_core', 'vllm::kda_attention', 'vllm::sparse_attn_indexer'], 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.FULL_AND_PIECEWISE: (2, 1)>, 'cudagraph_num_of_warmups': 1, 'cudagraph_capture_sizes': [1, 2, 4, 8, 16, 24, 32, 40, 48, 56, 64, 72, 80, 88, 96, 104, 112, 120, 128, 136, 144, 152, 160, 168, 176, 184, 192, 200, 208, 216, 224, 232, 240, 248, 256, 272, 288, 304, 320, 336, 352, 368, 384, 400, 416, 432, 448, 464, 480, 496, 512], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 512, 'local_cache_dir': None}
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:21 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://172.17.32.103:44989 backend=nccl
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:21 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:25 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-0.5B-Instruct...
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:35 [cuda.py:418] Valid backends: ['FLASH_ATTN', 'FLASHINFER', 'TRITON_ATTN', 'FLEX_ATTENTION']
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:35 [cuda.py:427] Using FLASH_ATTN backend.
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:36 [weight_utils.py:481] No model.safetensors.index.json found in remote.
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.32it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.32it/s]
(EngineCore_DP0 pid=2787353) 
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:36 [default_loader.py:314] Loading weights took 0.21 seconds
(EngineCore_DP0 pid=2787353) INFO 12-24 22:56:37 [gpu_model_runner.py:3338] Model loading took 0.9269 GiB memory and 11.264474 seconds
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:17 [backends.py:631] Using cache directory: /n/fs/vision-mix/jl0796/vllm/torch_compile_cache/adbeadfbd0/rank_0_0/backbone for vLLM's torch.compile
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:17 [backends.py:647] Dynamo bytecode transform time: 40.26 s
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:20 [backends.py:210] Directly load the compiled graph(s) for dynamic shape from the cache, took 2.494 s
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:21 [monitor.py:34] torch.compile takes 42.76 s in total
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:22 [gpu_worker.py:359] Available KV cache memory: 37.59 GiB
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:22 [kv_cache_utils.py:1229] GPU KV cache size: 3,284,656 tokens
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:22 [kv_cache_utils.py:1234] Maximum concurrency for 32,768 tokens per request: 100.24x
Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████████████████████████████████| 51/51 [00:01<00:00, 32.02it/s]
Capturing CUDA graphs (decode, FULL): 100%|█████████████████████████████████████████████████████████| 35/35 [00:00<00:00, 36.87it/s]
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:26 [gpu_model_runner.py:4244] Graph capturing finished in 3 secs, took 0.46 GiB
(EngineCore_DP0 pid=2787353) INFO 12-24 22:57:26 [core.py:250] init engine (profile, create kv cache, warmup model) took 48.81 seconds
INFO 12-24 22:57:27 [llm.py:352] Supported tasks: ['generate']
Adding requests: 100%|███████████████████████████████████████████████████████████████████████████████| 3/3 [00:00<00:00, 141.77it/s]
Processed prompts: 100%|████████████████████| 3/3 [00:00<00:00,  7.78it/s, est. speed input: 12009.23 toks/s, output: 817.02 toks/s]

Num samples: 3

Metrics:
  accuracy: 0.6667
  precision: 0.6667
  recall: 1.0000
  f1: 0.8000
  parse_success_rate: 1.0000

Prediction breakdown:
  Accept: 3
  Reject: 0
  Failed to parse: 0

============================================================
ALL TESTS COMPLETED
============================================================